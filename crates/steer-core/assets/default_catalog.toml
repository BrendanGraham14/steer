# Default catalog containing both providers and models

[[providers]]
id = "anthropic"
name = "Anthropic"
api_format = "anthropic"
auth_schemes = ["api-key", "oauth2"]

[[providers]]
id = "openai"
name = "OpenAI"
api_format = "openai-responses"
auth_schemes = ["api-key", "oauth2"]

[[providers]]
id = "google"
name = "Google"
api_format = "google"
auth_schemes = ["api-key"]

[[providers]]
id = "xai"
name = "xAI"
api_format = "xai"
auth_schemes = ["api-key"]

[[models]]
provider = "anthropic"
id = "claude-haiku-4-5"
aliases = ["haiku"]
recommended = false
context_window_tokens = 200000
[models.parameters]
max_output_tokens = 32_000
thinking_config.enabled = true
thinking_config.budget_tokens = 16_000

[[models]]
provider = "anthropic"
id = "claude-sonnet-4-5"
aliases = ["sonnet"]
recommended = true
context_window_tokens = 200000
[models.parameters]
max_output_tokens = 32_000
thinking_config.enabled = true
thinking_config.budget_tokens = 16_000

[[models]]
provider = "anthropic"
id = "claude-opus-4-5"
aliases = ["opus-4-5"]
recommended = false
context_window_tokens = 200000
[models.parameters]
max_output_tokens = 32_000
thinking_config.enabled = true
thinking_config.budget_tokens = 16_000

[[models]]
provider = "anthropic"
id = "claude-opus-4-6"
aliases = ["opus", "opus-4-6"]
recommended = true
context_window_tokens = 200000
[models.parameters]
max_output_tokens = 32_000
thinking_config.enabled = true
thinking_config.budget_tokens = 16_000

[[models]]
provider = "openai"
id = "gpt-5-nano-2025-08-07"
aliases = ["gpt-5-nano", "gpt5-nano"]
recommended = false
context_window_tokens = 400000
[models.parameters]
max_output_tokens = 128_000
thinking_config.enabled = true
thinking_config.effort = "medium"

[[models]]
provider = "openai"
id = "gpt-5-mini-2025-08-07"
aliases = ["gpt-5-mini", "gpt5-mini"]
recommended = false
context_window_tokens = 400000
[models.parameters]
max_output_tokens = 128_000
thinking_config.enabled = true
thinking_config.effort = "medium"


[[models]]
provider = "openai"
id = "gpt-5.2-2025-12-11"
aliases = ["gpt-5.2", "gpt5.2", "gpt52", "gpt-5.2-xhigh"]
recommended = false
context_window_tokens = 400000
[models.parameters]
max_output_tokens = 128_000
thinking_config.enabled = true
thinking_config.effort = "x-high"

[[models]]
provider = "openai"
id = "gpt-5.3-codex"
aliases = ["gpt5.3-codex", "gpt53-codex", "codex"]
recommended = true
context_window_tokens = 400000
[models.parameters]
max_output_tokens = 128_000
thinking_config.enabled = true
thinking_config.effort = "x-high"

[[models]]
provider = "openai"
id = "gpt-5.2-codex"
aliases = ["gpt5.2-codex", "gpt52-codex"]
recommended = false
context_window_tokens = 400000
[models.parameters]
max_output_tokens = 128_000
thinking_config.enabled = true
thinking_config.effort = "x-high"

[[models]]
provider = "openai"
id = "gpt-5.1-codex-max"
aliases = ["gpt5.1-codex-max", "gpt51-codex-max"]
recommended = false
context_window_tokens = 400000
[models.parameters]
max_output_tokens = 128_000
thinking_config.enabled = true
thinking_config.effort = "x-high"

[[models]]
provider = "google"
id = "gemini-3-flash-preview"
aliases = ["gemini-flash", "flash"]
recommended = true
context_window_tokens = 1048576
[models.parameters]
max_output_tokens = 65_536

[[models]]
provider = "google"
id = "gemini-3-pro-preview"
aliases = ["gemini-pro", "gemini"]
recommended = true
context_window_tokens = 1048576
[models.parameters]
max_output_tokens = 65_536
thinking_config.enabled = true
thinking_config.include_thoughts = true
thinking_config.budget_tokens = 8192

[[models]]
provider = "xai"
id = "grok-4-1-fast-reasoning"
aliases = ["grok-4-1-fast", "grok-4-1", "grok-4", "grok"]
recommended = true
context_window_tokens = 256000
[models.parameters]
max_output_tokens = 32_768

